{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit ('base': conda)",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Overall strategy:\n",
    "\n",
    "Knowing that every tenth sample from the training data was labeled, the data was extrapolated to include the entire train_time_series set. \n",
    "\n",
    "A simple rolling window was used to find the mean value of the window samples(for all three axis). The NaNs that this produced were replaced with the first value returned by the rolling window.\n",
    "\n",
    "To help out the final classifier, a visual inspection was done on the mean transformed data to locate the \"Standing\" labels since the \"standing\" samples are quite obvious to the naked eye. A simple boolean was used to find if the values of the 'x' feature met a certain threshold value. This was then added as three extra columns to reinforce the 'standing' datapoint. (a weight could have been used instead I presume). \n",
    "\n",
    "The classifier used was a stacking classifier as to use a Random Forrest Classifier, and then feed that into a KNN classifier in hopes of cleaning up any erractic, one-off predictions.\n",
    "\n",
    "The results seemed very good for the training/test split. However, there is probably a substantial amount of overfitting going on due to the fact that the result accuracy trends with the test to train data ratio.  I tried playing with the classifiers but I could not keep it from overfitting.\n",
    "\n",
    "Thanks for reviewing and I am excited to see everyones solutions!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "#start timer\n",
    "start = time.time()\n",
    "\n",
    "train_labels = pd.read_csv((\"train_labels.csv\"), index_col=0)\n",
    "train_time_series = pd.read_csv((\"train_time_series.csv\"), index_col=0)\n",
    "test_time_series = pd.read_csv((\"test_time_series.csv\"), index_col=0)\n",
    "test_labels = pd.read_csv((\"test_labels.csv\"),index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final test data set\n",
    "test_time = test_time_series[['x','y','z']]\n",
    "\n",
    "#since every tenth sample is labled, extrapolated to label the entire large dataset for the purpose of having more data to work with.\n",
    "lrgset = train_time_series[['x','y','z']]\n",
    "lrgset['label'] = train_labels[['label']].copy()\n",
    "lrgset.label[0:100] = 1.0\n",
    "lrgset.label[105:980] = 2.0\n",
    "lrgset.label[1000:1350] = 4.0\n",
    "lrgset.label[1380:1530] = 1.0\n",
    "lrgset.label[1540:1870] = 3.0\n",
    "lrgset.label[1880:2350] = 2.0\n",
    "lrgset.label[2360:2890] = 3.0\n",
    "lrgset.label[2900:3660] = 2.0\n",
    "lrgset.label[3670:3760] = 4.0\n",
    "lrgset = lrgset[pd.notnull(lrgset['label'])]\n",
    "\n",
    "#Standardizing and resetting index of the large dataset and keeping it isolated from X,y.\n",
    "X = lrgset[['x','y','z']]\n",
    "y = lrgset[['label']]\n",
    "X = X.reset_index().drop(columns='index')\n",
    "y = y.reset_index().drop(columns='index')\n",
    "\n",
    "#y axis at rest is -1 for 1G from the force from gravity. Added it back in to standardize further. \n",
    "X['y'] = X.y + 1\n",
    "\n",
    "#same actions taken on final test data as above\n",
    "test_time['y'] = test_time.y + 1\n",
    "test_time = test_time.reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_rolling_mean(X,y,windows,remove_nan=True):\n",
    "    '''This function will get the mean average of a moving window on a dataset X. 'y' is the labels for X. 'windows' will give the size of the rolling window. You may pass a list of intergers to itterate the data and take multiple means. 'remove_nan' default is True, which will replace nans at the beggining of the list with the value of the first mean value in the new list. If set to false, the function will return the original label list in addition to an index of nans'''    \n",
    "    for i in range(len(windows)):\n",
    "        win = windows[i]\n",
    "        X_mean = X.rolling(win).mean()\n",
    "        X_mean.x.iloc[0:win] = X_mean.x.iloc[win+1]\n",
    "        X_mean.y.iloc[0:win] = X_mean.y.iloc[win+1]\n",
    "        X_mean.z.iloc[0:win] = X_mean.z.iloc[win+1]\n",
    "        X = X_mean\n",
    "    nans = np.where(np.isnan(X_mean))[0]\n",
    "    if remove_nan == True:\n",
    "        y = y.drop(nans)\n",
    "        X = X.drop(nans)\n",
    "    else:\n",
    "        return(X,y,nans)\n",
    "    #X_mean = X_mean[~np.isnan(X_mean)]\n",
    "    return(X,y)\n",
    "\n",
    "def data_rolling_mean_test(X,windows,remove_nan=True):\n",
    "    '''this function does the same as data_rolling_mean except that it does not expect y- Used for the final test data.'''\n",
    "    for i in range(len(windows)):\n",
    "        win = windows[i]\n",
    "        X_mean = X.rolling(win).mean()\n",
    "        X_mean.x.iloc[0:win] = X_mean.x.iloc[win+1]\n",
    "        X_mean.y.iloc[0:win] = X_mean.y.iloc[win+1]\n",
    "        X_mean.z.iloc[0:win] = X_mean.z.iloc[win+1]\n",
    "        X = X_mean\n",
    "    nans = np.where(np.isnan(X_mean))[0]\n",
    "    if remove_nan == True:\n",
    "        X = X.drop(nans)\n",
    "    else:\n",
    "        return(X,nans)\n",
    "    return(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passing train data and test data into mean functions from above.  X_known,y_known is the data to be trained with.\n",
    "\n",
    "X_known,y_known= data_rolling_mean(X,y,[80,20],remove_nan=True)\n",
    "X_test = data_rolling_mean_test(test_time,[80,20],remove_nan=True)\n",
    "\n",
    "#Adding extra columns to help the classifier identify label \"1\".  Threshold value based on visually inspecting data.\n",
    "X_test['xones'] = False\n",
    "X_test['yones'] = False\n",
    "X_test['zones'] = False\n",
    "X_test['xones'].where(X_test.x > .090,other=True,inplace=True)\n",
    "X_test['yones'].where(X_test.x > .090,other=True,inplace=True)\n",
    "X_test['zones'].where(X_test.x > .090,other=True,inplace=True)\n",
    "X_known['xones'] = False\n",
    "X_known['yones'] = False\n",
    "X_known['zones'] = False\n",
    "X_known['xones'].where(X_known.x > .090,other=True,inplace=True)\n",
    "X_known['yones'].where(X_known.x > .090,other=True,inplace=True)\n",
    "X_known['zones'].where(X_known.x > .090,other=True,inplace=True)\n"
   ]
  },
  {
   "source": [
    "Below, you will find the training cell. Here, after tweaking some of the variables, and trying different classifiers, it seems that the accuracy tops out at about 96-97%.  I feel that there is probably some overfitting present."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9598173515981735\n"
     ]
    }
   ],
   "source": [
    "#This cell is for training the classifier only. \n",
    "\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(X_known,y_known,train_size=.7, shuffle=True)\n",
    "\n",
    "#Init classifiers\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "rfc = RandomForestClassifier(n_estimators=1000, n_jobs=-1, criterion='entropy')\n",
    "\n",
    "#List of estimators used in the stack.\n",
    "estimators = [('rf',rfc),('svr', make_pipeline(StandardScaler(),LinearSVC(random_state=42)))]\n",
    "\n",
    "#Init the stacking classifier with estimators from above\n",
    "stack_clf = StackingClassifier(estimators=estimators, final_estimator = knn)\n",
    "\n",
    "#Fit, predict, and print accuracy.\n",
    "stack_clf.fit(xtrain,ytrain)\n",
    "p = stack_clf.predict(xtest)\n",
    "\n",
    "print(accuracy_score(ytest, p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The total time for the code to complete was: 42.3 seconds\n"
     ]
    }
   ],
   "source": [
    "#This cell is the final test.  \n",
    "\n",
    "#Re-init the classifiers\n",
    "rfc = RandomForestClassifier(n_estimators=1000, n_jobs=-1, criterion='entropy')\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "estimators = [('rf', RandomForestClassifier(n_estimators=1000, random_state=42)),('svr', make_pipeline(StandardScaler(),LinearSVC(random_state=42)))]\n",
    "stack_clf = StackingClassifier(estimators=estimators, final_estimator = knn)\n",
    "\n",
    "#Fit, predict and stop timer\n",
    "stack_clf.fit(X_known,y_known)\n",
    "prediction = stack_clf.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"The total time for the code to complete was:\",round(time.time()-start, ndigits=2), \"seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving every 10th test predictions to the test_label dataframe and saving as csv.\n",
    "predicition = np.array(prediction)\n",
    "test_labels['label'] = prediction[::10,]\n",
    "test_labels.to_csv(os.path.abspath('test_labels.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
